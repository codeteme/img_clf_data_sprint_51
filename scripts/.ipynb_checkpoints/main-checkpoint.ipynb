{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of the exponential increase and computer power coupled with generation and storage of big data, the field of AI has taken the spot light in both academic and industry. In particular we have advacements in deep learning(explained further in the upcoming section) and meaningful high impact applications across all sorts of field. Deep Learning models are being used to model extremely complicated networks, make accurate regression and classification predictions, and even generate art. In this tutorial, we cover a particular type of Deep Learning architecutre called Convlutional Neural Network to build an animal classification model. With successful completion of this tutorial, it will be easy to imagine the array of subjects this image classifiction could be used at. For instance, identifing cancerous cells, self-driving car technology, security cameras, iris recognition improvement and so on.\n",
    "\n",
    "That said within the grander context of AI, Deep Learning can be thouht as a subset of Machine Learning which itself is a subset of AI. \n",
    "\n",
    "<img src=\"../img/ai_ml_dl.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook tutorial, we will build a model to classify images into one of five classes: mucca (cow), pecora (sheep), elefante (elephant), farfalla (butterfly) and scoiattolo (squirrel)\n",
    "\n",
    "The tutorial is sectioned as follows: \n",
    "1. Intro to Deep Learning\n",
    "* Convolutional Neural Networks\n",
    "2. Data Wrangling\n",
    "* Handling way too messy folder structures\n",
    "* Data collection\n",
    "* Data cleaning\n",
    "3. Data Exploration \n",
    "* TF dataset creation \n",
    "* Data visualization\n",
    "* Dealing with imbalanced dataset \n",
    "* Data augementation \n",
    "4. Modeling\n",
    "* TensorBoard for visualization\n",
    "* Dealing with overfitting\n",
    "5. Additional Resources</br></br>\n",
    "6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Intro to Deep Learning\n",
    "Artifical Neutral Networks(ANN) at the core of deep learning. ANNs are inspired by the netwoks of biological neurons found in our brains. We'll not go into detail about the histotry and nature of ANNs but it suffices to say they are fundamentally different from traditional Machine Learning models. \n",
    "\n",
    "One of the simplest ANN architectures is called the Perceptron. Below if a figure of a perceptron. \n",
    "\n",
    "<img src=\"../img/perceptron.png\" style=\"width:300px; height: 200px\"/>\n",
    "\n",
    "\n",
    "The inputs and output are numbers (instead of binary on/off values), and each input connection is associated with a weight. The unit computes a weighted sum of its inputs (z = w1 x1 + w2 x2 + ⋯ + wn xn = x⊺ w), then applies a step function to that sum and outputs the result: hw(x) = step(z), where z = x⊺ w. All of this is to say for given inpute the neuron process it and returns an output.\n",
    "\n",
    "\n",
    "\n",
    "We should not that neurons are the units of ANNs. An amalgam of these neurons creates a layer. As such we have input layers and output layers. Each ANN model contains hidden layers as well. The more the number of the layers, the deeper the neural network, hence the phrase \"Deep Learning\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Convolutional Neural Networks\n",
    "\n",
    "In this project we'll employ a type of ANNs called Convolutional Neural Networks(CNNs). CNNs are widely used in image classification tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to train a Deep Learning model that is trained on thousands of labeled data and is able to predict an image into one of said five classes. The project was set up as a challenge at [DPHI](https://dphi.tech/challenges/data-sprint-51-predict-the-image-of-the-species/167/overview/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Collection\n",
    "\n",
    "#### 2.1 Handling way too messy folder structures\n",
    "\n",
    "Before we start writing our code, it's a good practice to set up our directory structure. We will use the widely used [cookie clutter](https://drivendata.github.io/cookiecutter-data-science/) approach. We will modify the recommended structure for our purposes. We won't have any interim files nor will we save our processed file or use external data. Instead, we will just have a directory named raw which will contain our downloaded file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is stored and shared on a Google Drive folder as a zip file. We will automate the task of downloading, moving to a preffered directory, unzipping and renaming tasks using Python. We will need two libraries for these tasks: gdown and os. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the following cell to install the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install seaborn\n",
    "# !pip install gdown\n",
    "# !pip install tensorflow\n",
    "# !pip install keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are a fan of `Zen` mode on VSCode you might want to uncomment, run and see if you like the workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Getting the data\n",
    "\n",
    "The given dataset is located at https://drive.google.com/file/d/176E-pLhoxTgWsJ3MeoJQV_GXczIA6g8D/view?usp=sharing\n",
    "We can either download the zipped file from our brower or use the library gdown and download the file using python. \n",
    "We choose the latter option. The gdown library requires that we remove extraneous information in the url including view and usp parameters. We only need the id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "# Download the file to the current working directory\n",
    "url = 'https://drive.google.com/uc?id=176E-pLhoxTgWsJ3MeoJQV_GXczIA6g8D'\n",
    "output = '../data/raw/animal_ds.tgz'\n",
    "gdown.download(url, output, quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file and move it to the correct directory.\n",
    "!unzip '../data/raw/animal_ds.tgz' -d ../data/raw \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the zipped file\n",
    "!rm ../data/raw/animal_ds.tgz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data sub-directory is created and contains the data, we will save its path for future use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "data_dir = \"../data/raw/animal_dataset_intermediate/train\"\n",
    "data_dir = pathlib.Path(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each one of the sub-folders under the train sub-sub-directory contain the '_train' substring in their name. We will use the os library to locate and rename the folders. We do so because tensorflow automatically extracts the class names from the folder names. This will explained after a few cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_files(): \n",
    "    for filename in os.listdir(data_dir):\n",
    "        full_filepath = os.path.join(data_dir, filename)\n",
    "        os.rename(full_filepath, full_filepath[:-6])\n",
    "rename_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only other action we will perform on our raw file is deletion. We only want to include images with certain file formats(e.g. *.jpg and *jpeg) we can easily work with ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-22T14:38:49.927008Z",
     "iopub.status.idle": "2021-10-22T14:38:49.927780Z",
     "shell.execute_reply": "2021-10-22T14:38:49.927556Z",
     "shell.execute_reply.started": "2021-10-22T14:38:49.927530Z"
    }
   },
   "outputs": [],
   "source": [
    "num_skipped = 0\n",
    "for folder_name in (\"elefante\", \"farfalla\", \"mucca\", \"pecora\", \"scoiattolo\"):\n",
    "    folder_path = os.path.join(data_dir, folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        if not is_jfif:\n",
    "            num_skipped += 1\n",
    "            # Delete corrupted image\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above deleted files if they are not of the type we can work with. It also printed the number of the files deleted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the number of images available in the train sub-directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T14:39:00.021176Z",
     "iopub.status.busy": "2021-10-22T14:39:00.020370Z",
     "iopub.status.idle": "2021-10-22T14:39:01.635489Z",
     "shell.execute_reply": "2021-10-22T14:39:01.634679Z",
     "shell.execute_reply.started": "2021-10-22T14:39:00.021127Z"
    }
   },
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')) + list(data_dir.glob('*/*.jpeg')))\n",
    "print(\"Imported image_count: \", image_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reading the files in a manner suitable for our modeling, we will view the first file just to see if it's actually an image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import PIL.Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephant = list(data_dir.glob('elefante/*'))\n",
    "PIL.Image.open(str(elephant[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Exploration\n",
    "\n",
    "We will be using Tensorflow a deep learning library. We will also be using Keras, an API that let's as access Tensorflow. \n",
    "\n",
    "(TensorFlow)[https://www.tensorflow.org/] is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks.\n",
    "\n",
    "[Keras](https://keras.io/) is a high-level Deep Learning API that makes it easy to build, train, evaluate, and execute all sorts of neural networks. It was developed by (François Chollet)[https://fchollet.com/] as an open source project in March 2015. It quickly gained popularity, owing to its ease of use, flexibility, and beautiful design. \n",
    "\n",
    "FYI: The most popular Deep Learning library, after Keras and TensorFlow, is Facebook’s PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T14:38:52.737915Z",
     "iopub.status.busy": "2021-10-22T14:38:52.737383Z",
     "iopub.status.idle": "2021-10-22T14:38:57.514472Z",
     "shell.execute_reply": "2021-10-22T14:38:57.513714Z",
     "shell.execute_reply.started": "2021-10-22T14:38:52.737873Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any Machine Learning project, we always have two distinct sets of our data. *Training set* and *Testing set*. Sometimes we have a third set called *Validation set* \n",
    "\n",
    "* Training set - contains the majority of the data and is used to train our model. The percentage of data allocated to this set can vary. Depending on the size of our data, we might allocate 70% - 90% of our dataset. The more data we have the more percentage we can allocate to our training set. \n",
    "* Validation set - much like training set, it contains data with both features and lables. It is used to measure the accuracy of the model i.e. how good does the model predict unseen data? \n",
    "* Testing set - This is feature only data for which our model will predict labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 TF dataset creation\n",
    "\n",
    "The`image_dataset_from_directory`(main_directory, labels='inferred') from Keras will return a tf.data.Dataset that yields batches of images from the subdirectories elefant, ..., scoiattolo. together with labels 0 to 4 (0 corresponding to elefante and 4 corresponding to scoiattolo).\n",
    "\n",
    "It supports the following image formats: jpeg, png, bmp, gif. Hence the need to remove unsupported format which we have already done.\n",
    "\n",
    "The`image_dataset_from_directory` accepts dataset folder divides it up to training and validation set. More importantly, the images are assigned into batches. In our case, we set the batch size to 32. If you have a low performing machine you can set it to 16. The image height and image width are set following the instruction from the compeition. The origianl shpae of the images is 256 by 256 so we will stick with that. \n",
    "\n",
    "<img src=\"../img/CNN_input_shape.png\" style=\"width:400px; height: 300px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T14:39:08.200873Z",
     "iopub.status.busy": "2021-10-22T14:39:08.200117Z",
     "iopub.status.idle": "2021-10-22T14:39:20.608190Z",
     "shell.execute_reply": "2021-10-22T14:39:20.607392Z",
     "shell.execute_reply.started": "2021-10-22T14:39:08.200817Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2, # We train our model using 80% of the train_ds and test on the remaining 20%.\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `image_dataset_from_directory` method created a TF object from our files. It also inferred the numbr of classes in both the training and validation sets, correctly so. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T14:40:11.802160Z",
     "iopub.status.busy": "2021-10-22T14:40:11.801790Z",
     "iopub.status.idle": "2021-10-22T14:40:11.814454Z",
     "shell.execute_reply": "2021-10-22T14:40:11.813774Z",
     "shell.execute_reply.started": "2021-10-22T14:40:11.802120Z"
    }
   },
   "outputs": [],
   "source": [
    "class_names = ['elefante', 'farfalla', 'mucca', 'pecora', 'scoiattolo']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Data visualization\n",
    "\n",
    "We can read and generate images from our TF object. Data visualization is not a one-off task. We are going to generate different figures as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `take()` methods accepts an float number that refers to the batch. Batching starts at 1 in TF. So, .take(1) refers to the first batch containing 32 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the trickiest challenges of working with images is resizing shapes. \n",
    "Note: We are treating the images as TF objects contained in batched datasets \n",
    "\n",
    "Despite the convenience, batched datasets can be a bit tricky especially when we want to apply changes on individual images. Here will try to resample our data because we have an imbalanced dataset. To do so, we first iterate through each batch and save each image as a numpy array and lables as another numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images(x) and lables(y) of a given batchdatasets\n",
    "def get_image_label(ds): \n",
    "    x_train_ = []\n",
    "    y_train_ = []\n",
    "    for element in ds.as_numpy_iterator(): \n",
    "        x_train_.append(element[0])\n",
    "        y_train_.append(element[1])\n",
    "    x_train = np.concatenate(x_train_)\n",
    "    y_train = np.concatenate(y_train_)\n",
    "    \n",
    "    return (x_train, y_train)\n",
    "\n",
    "x_train, y_train = get_image_label(train_ds)\n",
    "x_val, y_val = get_image_label(val_ds)\n",
    "\n",
    "print(type(x_train), type(y_train))\n",
    "print(x_train.shape, y_train.shape, x_train.ndim)\n",
    "print(x_val.shape, y_val.shape, x_val.ndim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to test objects and their attributes especially when casting them. In fact, in the previous cell we not only changes the dtype of the object we are working with but also which class it belongs to i.e. from TF batched dataset to numpy arrays. We expect that x_train which is the array for the feature. It should be four-dimensional as it the original object contained a four-dimensional object (batch_size, img_height, img_width, xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(x_train, (np.ndarray, np.generic))\n",
    "assert isinstance(y_train, (np.ndarray, np.generic))\n",
    "assert isinstance(x_val, (np.ndarray, np.generic))\n",
    "assert isinstance(y_val, (np.ndarray, np.generic))\n",
    "\n",
    "assert x_train.ndim, x_val == 4\n",
    "assert y_train.ndim, y_train.ndim == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Dealing with imbalanced dataset\n",
    "\n",
    "We can check whether our data is imbalanced by looking the number of images each class contains. To do so, we will use seaborn library to produce a data visualization. Our classes are categorical valus and their counts are continous, so we will use a bar graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Data viz to show data imbalance - bar graph\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "splot = sns.countplot(x = y_train)\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "    ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "splot.set_xticklabels(class_names)\n",
    "splot.set_xlabel(\"Classes\")\n",
    "splot.set_ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to deal with an imbalanced dataset: \n",
    "\n",
    "1. We can assign a larger penalty to wrong predictions on the minority class. \n",
    "2. We can upsample the minority class, \n",
    "3. We can downsample the majority class\n",
    "4. We can generate synthetic training examples.\n",
    "\n",
    "The most widely used algorithm for synthetic training data generation is Synthetic Minority Oversampling Technique (SMOTE). \n",
    "\n",
    "Of all the last point is probably the most complex to understand. Nonetheless, it is already implemented in imbalanced-learn a Python library that is entirely focused on imbalanced datasets. We are then at the luxury of using this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample \n",
    "\n",
    "print(\"-\" * 70)\n",
    "print('Input shape before resampling: ' ,x_train.shape, y_train.shape)\n",
    "\n",
    "#..reshape (flatten) x_train for SMOTE resampling\n",
    "nsamples, k, nx, ny = x_train.shape\n",
    "x_train = x_train.reshape((nsamples,k*nx*ny))\n",
    "x_train.shape\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='all')\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print('Input shape after sampling: ' ,x_train.shape, y_train.shape)\n",
    "print('Class distribution after over-sampling: ')\n",
    "for i in range(len(class_names)):\n",
    "    print(f'Number of class {class_names[i]} examples before:{x_train[y_train == i].shape[0]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we used SMOTE to resample our data, we can check if our training dataset contains equal number of images across the five classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "splot = sns.countplot(x = y_train)\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "    ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "splot.set_xticklabels(class_names)\n",
    "splot.set_xlabel(\"Classes\")\n",
    "splot.set_ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed our resampling, we can now restore the images to their original four-dimensional shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return to original 4D shape\n",
    "x_train = x_train.reshape(7325, k, nx, ny)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance enhancement\n",
    "\n",
    "Training deep learning models can be a computationally expensive task. There are several ways to better our modeling performance, one is prefetching. Prefetching generally refers to the use of a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The details are not particularly of interest to us at the moment. Besides, we can let scikit-learn optimize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another difference between traditional machine learning and deep learning is that deep learning requires huge amounts of data. There is no particular number we're looking at to decide whether we should use one or the other. Generally speaking so he wants quite a bit of data in either case and more so for deep learning.\n",
    "\n",
    "That being said around dataset contains around 8000 images and there is not necessarily enough to train a deep learning model or at least a fairly accurate one. In addition, in the real world scenario, as opposed to certain conditions our target application may exist in a variety of conditions, such as different orientation, location, scale, brightness etc. To account for these situations, we can use data augementation, which is a technique of training our neural network with additional synthetically modified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T14:41:07.352482Z",
     "iopub.status.busy": "2021-10-22T14:41:07.351747Z",
     "iopub.status.idle": "2021-10-22T14:41:07.383205Z",
     "shell.execute_reply": "2021-10-22T14:41:07.382477Z",
     "shell.execute_reply.started": "2021-10-22T14:41:07.352440Z"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\", input_shape=(img_height, img_width,3)),\n",
    "    layers.RandomRotation(0.1),layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast((0.1, 0.9)),\n",
    "    ])\n",
    "\n",
    "assert(x_train.ndim == 4) # Check if augementation affected shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeling\n",
    "Our dataset has been cleaned, resized, balanced, and augemented. Now, we can safely move to the modeling phase. Here we define our Neural Network structure. By defining we mean choosing the architeture along with number of neurons and layers, and choosing activation function. We then compile our model which includes choosing the optimizer, loss function and metrics. We have defined neurons and layers but what are optimizers and loss functions?\n",
    "\n",
    "As opposed to traditional ML, in Deep Learning we let the neurons find out the weight and biases of each neuron. Naturally, then we need a methods to decide which of these weight and biase parameters result in a more accurate model. There are two components that let's us do exactly that. Loss functions computes the distance between the current output of the algorithm and the expected output, then return a single value that we can compare with other weight and biase choices. Optimizers are mathematical functions which are dependent on model's learnable parameters i.e. weights & biases.\n",
    "\n",
    "Delving into all the loss functions and optimizers is beyond the scope of this tutorial. We just need to note that we'll use Adam optimiser and SparseCategoricalCrossentropy because we have multi-class labels.\n",
    "\n",
    "#### Tensorboard for visualization\n",
    "\n",
    "TensorBoard is a great interactive visualization tool that can show the learning curves during training, compare learning curves between multiple runs, visualize the computation graph, analyze training statistics, view images generated by the model, visualize complex multidimensional data projected down to 3D and more! \n",
    "\n",
    "TensorBoard might launch in a different port so you don't have to worry about your running jupyter notebooks. \n",
    "Below are figures of screenshots taken while training.\n",
    "\n",
    "<img src=\"../img/screenshot_tensorboard_visualization.png\" style=\"width:1100px; height: 600px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice here we have pooling features. A definition from the popular *machinelearningmastery* blog can help with that.\n",
    "\n",
    "<blockquote>A problem with the output feature maps is that they are sensitive to the location of the features in the input. One approach to address this sensitivity is to down sample the feature maps. This has the effect of making the resulting down sampled feature maps more robust to changes in the position of the feature in the image, referred to by the technical phrase “local translation invariance.”\n",
    "</blockquote> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  data_augmentation,\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2), # Dropout https://www.tensorflow.org/tutorials/images/classification#dropout\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy']\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with overfitting\n",
    "\n",
    "Two callbacks options to implement early Stopping to avoid overfitting. \n",
    "\n",
    "1. keras.callbacks.ModelCheckpoint() saves the model when its performance on the validation set is the best so far\n",
    "\n",
    "2. keras.callbacks.EarlyStopping() interrupts training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model.\n",
    "\n",
    "It's possible to  combine both callbacks to save checkpoints of your model (in case the computer crashes) and interrupt training early when there is no more progress (to avoid wasting time and resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite our attempt to enhance the model traning performance, it might take us a lot of time to complete the training. If GPUs or TPUs are available then changing the runtime is highly recommended. If not we can load a saved model for our convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"saved_model/keras_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history =  model.fit(\n",
    "  x_train, y_train,\n",
    "  validation_data=(x_val, y_val),\n",
    "  epochs=epochs,\n",
    "  callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint_cb saves the best model. Just in case, though, we'll also save the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your model in the appropriate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../saved_model/best_model') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute your model on the validation set. If you load the saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_val, y_val, verbose=2) # ~78% accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the following cell to load a saved model with an approximate accuracy of 78%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = tf.keras.models.load_model('../my_keras_model.h5')\n",
    "# new_model.evaluate(x_val, y_val, verbose=2) # ~78% accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! We have a working model. Typically this would mark the end of our journey, but our aim is to predict a test set (one with no labels) and submit it to the competition. We will do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = \"../data/raw/animal_dataset_intermediate/test\"\n",
    "test_data_dir = pathlib.Path(test_data_dir)\n",
    "print(test_data_dir)\n",
    "\n",
    "image_count = len(list(test_data_dir.glob('*.jpg')) + list(test_data_dir.glob('*.jpeg')))\n",
    "print(\"Imported image_count: \", image_count)\n",
    "\n",
    "picture = list(test_data_dir.glob('*'))\n",
    "PIL.Image.open(str(picture[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge orgnizers want us to submit our predictions as a csv file wth headers filename and target. The order for the filename column is already given. We will iterate through the given `Testing_set_animals.csv` file predict the class and put in our predictions in the `submissions.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_filename = pd.read_csv(\"../data/raw/animal_dataset_intermediate/Testing_set_animals.csv\")\n",
    "\n",
    "classes = []\n",
    "\n",
    "df_len = df_submission_filename.shape[0]\n",
    "for i in range(df_len): \n",
    "    path = os.path.join(test_data_dir, df_submission_filename.loc[i][0])\n",
    "    img = tf.keras.utils.load_img(\n",
    "        path, grayscale=False, color_mode='rgb', target_size=(img_height, img_width),\n",
    "        interpolation='nearest'\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = img_array.reshape(1, k, nx, ny)\n",
    "    predict_img = model.predict(img_array) \n",
    "    classes_img = np.argmax(predict_img,axis=1)\n",
    "    classes.append(classes_img[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_filename['target'] = [class_names[num] for num in list(classes)]\n",
    "df_submission_filename.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_filename.to_csv(\"../submission/submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "Apart from the hyperlinks presented above and the refernces listed below, here are additonal resources you can check:\n",
    "- [Create your Own Image Classification Model using Python and Keras](https://www.analyticsvidhya.com/blog/2020/10/create-image-classification-model-python-keras/#h2_7)\n",
    "- [Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "- [Build your First Multi-Label Image Classification Model in Python](https://www.analyticsvidhya.com/blog/2019/04/build-first-multi-label-image-classification-model-python/)\n",
    "- [Top 4 Pre-Trained Models for Image Classification with Python Code](https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/)\n",
    "- [Complete Guide To Bidirectional LSTM (With Python Codes)](https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/)\n",
    "- [Advanced CNN Architectures](https://livebook.manning.com/book/grokking-deep-learning-for-computer-vision/chapter-5/v-3/9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Brownlee, J. (2019, July 5). Best practices for preparing and augmenting image data for cnns. Machine Learning Mastery. Retrieved October 28, 2021, from https://machinelearningmastery.com/best-practices-for-preparing-and-augmenting-image-data-for-convolutional-neural-networks/. \n",
    "\n",
    "Brownlee, J. (2020, August 14). What is the difference between test and validation datasets? Machine Learning Mastery. Retrieved October 28, 2021, from https://machinelearningmastery.com/difference-test-validation-datasets/. \n",
    "\n",
    "Brownlee, J. (2021, March 16). Smote for imbalanced classification with python. Machine Learning Mastery. Retrieved October 28, 2021, from https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/. \n",
    "\n",
    "Géron Aurélien. (2019). Hands-on machine learning with scikit-learn and tensorflow concepts, tools, and techniques to build Intelligent Systems. O'Reilly. \n",
    "\n",
    "Jupyter Notebook markdown tutorial. DataCamp Community. (n.d.). Retrieved October 28, 2021, from https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook. \n",
    "\n",
    "Keras documentation: Image Classification From Scratch. Keras. (n.d.). Retrieved October 28, 2021, from https://keras.io/examples/vision/image_classification_from_scratch/. \n",
    "\n",
    "Load and preprocess images &nbsp;: &nbsp; Tensorflow Core. TensorFlow. (n.d.). Retrieved October 28, 2021, from https://www.tensorflow.org/tutorials/load_data/images. \n",
    "\n",
    "Raschka, S., &amp; Mirjalili, V. (2019). Python machine learning: Machine learning and deep learning with python, scikit-learn, and tensorflow 2. Packt Publishing. \n",
    "\n",
    "Verma, S. (2021, October 5). Understanding input and output shapes in convolution neural network: Keras. Medium. Retrieved October 28, 2021, from https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d9279c439293cd56693e7c05115c53cbb1633ee63d7052d68a345e17a860490"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
